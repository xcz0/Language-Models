# configs/gpt.yaml
# --------------------------------------------------
# GPT (Generative Pre-trained Transformer) 模型的特定配置。
# 基于 OpenAI GPT-2 架构的仅解码器 Transformer 语言模型。
# 它会加载 base_config.yaml，然后用以下值覆盖默认设置。
# --------------------------------------------------

# 将输出保存到专门的目录
system:
  work_dir: 'out/gpt'

# GPT 模型的训练配置 - 相对复杂的模型需要更多训练步数
training:
  max_steps: 15000       # GPT 模型训练步数，比默认值稍多

# GPT 模型的关键架构参数
model:
  type: 'gpt'
  # 注意：block_size 和 vocab_size 由 DataModule 根据数据自动确定，无需在这里指定
  
  # Transformer 层数 - 控制模型深度
  n_layer: 6             # 6 层 Transformer 解码器块
  
  # 多头注意力机制的头数
  n_head: 6              # 6 个注意力头，需要能被 n_embd 整除
  
  # 词嵌入和隐藏状态的维度
  n_embd: 192            # 嵌入维度，192 维向量，确保能被 n_head 整除

# GPT 模型适合的优化器配置
optimizer:
  learning_rate: 0.0003  # 3e-4，比默认值稍低，更稳定
  weight_decay: 0.1      # 较高的权重衰减，有助于正则化
  params:
    betas: [0.9, 0.95]   # GPT-2 论文中推荐的 beta 值
    eps: 1e-8            # 数值稳定性
